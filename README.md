# multilingual_spam_sms_classification

This project aims to develop a spam filter using Naive Bayes, Neural Network, and BERT models. The project involves data preprocessing, model training, and evaluation.

## Data Preprocessing
The data used for this project is a collection of messages labeled as either spam or ham. The text of each email message is preprocessed by converting it to lowercase, removing special characters and numbers, and tokenizing it.

## Naive Bayes Model
A Naive Bayes classifier is a simple yet effective model for text classification. It is trained on the preprocessed email messages and used to predict whether new messages are spam or ham.

## Neural Network Model
A neural network model is a more complex model for text classification. It is trained on the preprocessed email messages and used to predict whether new messages are spam or ham.

## BERT Model
BERT is a powerful language model that can be used for text classification. It is trained on a massive dataset of text and code, and it can learn complex relationships between words and phrases.

## Evaluation
The performance of the three models is evaluated using accuracy, precision, recall, and F1-score.

## Results
The results of the evaluation show that the BERT model is the most accurate model, followed by the neural network model and the Naive Bayes model.

## Conclusion
The results of this project show that BERT is a powerful tool for spam filtering. However, the neural network model is also a viable option for spam filtering, especially if computational efficiency is a concern.

## Future Work
Future work could focus on improving the performance of the neural network model and exploring the use of other machine learning models for spam filtering.
